{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Convert 3D Poses to Action Videos using Diffusion Models\n",
    "\n",
    "This notebook converts sequences of 3D joint poses into action videos using diffusion models (AnimateDiff/ControlNet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install required libraries for pose-to-video generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q diffusers transformers accelerate torch torchvision opencv-python numpy pillow imageio tqdm\n",
    "!pip install -q xformers  # For memory efficient attention (optional but recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 2. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_cell"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    DDIMScheduler,\n",
    ")\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_data"
   },
   "source": [
    "## 3. Load Pose Data\n",
    "\n",
    "Upload your pose files to Colab or mount Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional - if your data is in Drive)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "# Alternative: Upload files directly\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Or specify path if data is already in Colab/Drive\n",
    "DATA_DIR = \"/content/drive/MyDrive/dataset/violence/violence_01\"  # Update this path\n",
    "# DATA_DIR = \"/content/pose_data\"  # Or use uploaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pose_loader"
   },
   "outputs": [],
   "source": [
    "def load_pose_data(file_path):\n",
    "    \"\"\"Load pose data from JSON, numpy, or other formats\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if file_path.suffix == '.json':\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Handle different JSON structures\n",
    "        if isinstance(data, list):\n",
    "            poses = []\n",
    "            for frame in data:\n",
    "                if isinstance(frame, dict) and 'joints' in frame:\n",
    "                    joints = np.array(frame['joints'])\n",
    "                elif isinstance(frame, dict) and 'pose' in frame:\n",
    "                    joints = np.array(frame['pose'])\n",
    "                elif isinstance(frame, list):\n",
    "                    joints = np.array(frame)\n",
    "                else:\n",
    "                    joints = np.array(list(frame.values()))\n",
    "                poses.append(joints)\n",
    "            return np.array(poses)\n",
    "        elif isinstance(data, dict):\n",
    "            if 'frames' in data:\n",
    "                return np.array(data['frames'])\n",
    "            elif 'poses' in data:\n",
    "                return np.array(data['poses'])\n",
    "            elif 'joints' in data:\n",
    "                return np.array(data['joints'])\n",
    "            else:\n",
    "                return np.array(list(data.values()))\n",
    "    \n",
    "    elif file_path.suffix == '.npy':\n",
    "        return np.load(file_path)\n",
    "    \n",
    "    elif file_path.suffix == '.npz':\n",
    "        data = np.load(file_path)\n",
    "        return data[list(data.keys())[0]]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {file_path.suffix}\")\n",
    "\n",
    "# Example: Load pose file\n",
    "# poses_3d = load_pose_data(\"path/to/pose_file.json\")\n",
    "# print(f\"Loaded poses shape: {poses_3d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "convert_poses"
   },
   "source": [
    "## 4. Convert 3D Poses to OpenPose Format\n",
    "\n",
    "Convert 3D poses to 2D OpenPose skeleton images for ControlNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "openpose_converter"
   },
   "outputs": [],
   "source": [
    "class PoseToOpenPose:\n",
    "    \"\"\"Convert 3D poses to 2D OpenPose format for ControlNet\"\"\"\n",
    "    \n",
    "    # Skeleton connections (adjust based on your joint structure)\n",
    "    SKELETON = [\n",
    "        [0, 1], [1, 2], [1, 5], [2, 3], [3, 4],  # Head, arms\n",
    "        [5, 6], [6, 7], [1, 8], [8, 9], [9, 10],  # Body, right leg\n",
    "        [10, 11], [8, 12], [12, 13], [13, 14],  # Left leg\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, image_size=(512, 512)):\n",
    "        self.image_size = image_size\n",
    "    \n",
    "    def project_to_2d(self, pose_3d):\n",
    "        \"\"\"Project 3D pose to 2D by taking XY coordinates\"\"\"\n",
    "        if pose_3d.shape[-1] >= 2:\n",
    "            return pose_3d[..., :2]\n",
    "        return pose_3d\n",
    "    \n",
    "    def draw_openpose(self, keypoints_2d):\n",
    "        \"\"\"Draw OpenPose skeleton on blank image\"\"\"\n",
    "        img = Image.new('RGB', self.image_size, color='black')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        if keypoints_2d.shape[0] > 0:\n",
    "            # Normalize to [0, 1]\n",
    "            keypoints = keypoints_2d.copy()\n",
    "            keypoints = keypoints - keypoints.min(axis=0)\n",
    "            max_val = keypoints.max()\n",
    "            if max_val > 0:\n",
    "                keypoints = keypoints / max_val\n",
    "            \n",
    "            # Scale to image size (with padding)\n",
    "            padding = 0.1\n",
    "            keypoints = keypoints * np.array([self.image_size[0] * (1 - 2*padding), \n",
    "                                             self.image_size[1] * (1 - 2*padding)])\n",
    "            keypoints = keypoints + np.array([self.image_size[0] * padding, \n",
    "                                             self.image_size[1] * padding])\n",
    "            \n",
    "            # Draw skeleton connections\n",
    "            for connection in self.SKELETON:\n",
    "                if connection[0] < len(keypoints) and connection[1] < len(keypoints):\n",
    "                    pt1 = tuple(keypoints[connection[0]].astype(int))\n",
    "                    pt2 = tuple(keypoints[connection[1]].astype(int))\n",
    "                    draw.line([pt1, pt2], fill='white', width=3)\n",
    "            \n",
    "            # Draw keypoints\n",
    "            for i, pt in enumerate(keypoints):\n",
    "                if i < len(keypoints):\n",
    "                    x, y = tuple(pt.astype(int))\n",
    "                    draw.ellipse([x-5, y-5, x+5, y+5], fill='white')\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def convert_sequence(self, poses_3d):\n",
    "        \"\"\"Convert sequence of 3D poses to OpenPose images\"\"\"\n",
    "        openpose_images = []\n",
    "        \n",
    "        for pose in poses_3d:\n",
    "            # Project to 2D\n",
    "            pose_2d = self.project_to_2d(pose)\n",
    "            # Draw skeleton\n",
    "            skeleton_img = self.draw_openpose(pose_2d)\n",
    "            openpose_images.append(skeleton_img)\n",
    "        \n",
    "        return openpose_images\n",
    "\n",
    "# Initialize converter\n",
    "pose_converter = PoseToOpenPose(image_size=(512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_poses"
   },
   "outputs": [],
   "source": [
    "# Visualize pose conversion (example)\n",
    "# Uncomment and adjust when you have pose data loaded\n",
    "\n",
    "# if 'poses_3d' in locals():\n",
    "#     # Show first frame\n",
    "#     control_images = pose_converter.convert_sequence(poses_3d[:5])  # First 5 frames\n",
    "#     fig, axes = plt.subplots(1, min(5, len(control_images)), figsize=(15, 3))\n",
    "#     if len(control_images) == 1:\n",
    "#         axes = [axes]\n",
    "#     for i, img in enumerate(control_images[:5]):\n",
    "#         axes[i].imshow(img)\n",
    "#         axes[i].axis('off')\n",
    "#         axes[i].set_title(f'Frame {i+1}')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_model"
   },
   "source": [
    "## 5. Load Diffusion Model\n",
    "\n",
    "Load ControlNet with OpenPose for pose-conditioned video generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_controlnet"
   },
   "outputs": [],
   "source": [
    "# Load ControlNet with OpenPose\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-openpose\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    safety_checker=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "\n",
    "# Enable memory efficient attention\n",
    "pipe.enable_model_cpu_offload()\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    print(\"XFormers enabled for memory efficiency\")\n",
    "except:\n",
    "    print(\"XFormers not available, using default attention\")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate_video"
   },
   "source": [
    "## 6. Generate Video from Poses\n",
    "\n",
    "Generate action video frames from the pose sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_frames"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PROMPT = \"a person performing a violent action, high quality, detailed, realistic, dynamic movement, cinematic\"\n",
    "NEGATIVE_PROMPT = \"blurry, low quality, distorted, cartoon, animation, static, low resolution\"\n",
    "NUM_INFERENCE_STEPS = 20  # More steps = better quality but slower\n",
    "GUIDANCE_SCALE = 7.5  # How closely to follow the prompt\n",
    "FPS = 8  # Frames per second for output video\n",
    "\n",
    "def generate_video_frames(control_images, prompt, negative_prompt, num_steps=20, guidance=7.5):\n",
    "    \"\"\"Generate video frames from control images\"\"\"\n",
    "    frames = []\n",
    "    \n",
    "    print(f\"Generating {len(control_images)} frames...\")\n",
    "    for i, control_img in enumerate(tqdm(control_images, desc=\"Generating frames\")):\n",
    "        # Add frame context to prompt for better temporal consistency\n",
    "        frame_prompt = f\"{prompt}, frame {i+1} of {len(control_images)}\"\n",
    "        \n",
    "        # Generate frame\n",
    "        result = pipe(\n",
    "            prompt=frame_prompt,\n",
    "            image=control_img,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=num_steps,\n",
    "            guidance_scale=guidance,\n",
    "        ).images[0]\n",
    "        \n",
    "        frames.append(result)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Example usage (uncomment when you have pose data):\n",
    "# control_images = pose_converter.convert_sequence(poses_3d)\n",
    "# video_frames = generate_video_frames(\n",
    "#     control_images,\n",
    "#     PROMPT,\n",
    "#     NEGATIVE_PROMPT,\n",
    "#     NUM_INFERENCE_STEPS,\n",
    "#     GUIDANCE_SCALE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_video"
   },
   "outputs": [],
   "source": [
    "def save_video(frames, output_path, fps=8):\n",
    "    \"\"\"Save frames as video file\"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Convert PIL images to numpy arrays\n",
    "    frame_arrays = [np.array(frame) for frame in frames]\n",
    "    \n",
    "    # Save as video using imageio\n",
    "    imageio.mimsave(\n",
    "        str(output_path),\n",
    "        frame_arrays,\n",
    "        fps=fps,\n",
    "        codec='libx264',\n",
    "        quality=8,\n",
    "    )\n",
    "    \n",
    "    print(f\"Video saved to: {output_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# save_video(video_frames, \"/content/output_video.mp4\", fps=FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "complete_pipeline"
   },
   "source": [
    "## 7. Complete Pipeline\n",
    "\n",
    "Run the complete pipeline from pose data to video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline"
   },
   "outputs": [],
   "source": [
    "# Complete pipeline\n",
    "def process_pose_file(pose_file_path, output_path, max_frames=None):\n",
    "    \"\"\"Complete pipeline: Load pose -> Convert to OpenPose -> Generate video -> Save\"\"\"\n",
    "    \n",
    "    # 1. Load pose data\n",
    "    print(f\"Loading pose data from: {pose_file_path}\")\n",
    "    poses_3d = load_pose_data(pose_file_path)\n",
    "    print(f\"  Loaded poses shape: {poses_3d.shape}\")\n",
    "    \n",
    "    # Limit frames if specified (for testing)\n",
    "    if max_frames and len(poses_3d) > max_frames:\n",
    "        poses_3d = poses_3d[:max_frames]\n",
    "        print(f\"  Limited to {max_frames} frames\")\n",
    "    \n",
    "    # 2. Normalize poses (center and scale)\n",
    "    # Center the pose\n",
    "    for i in range(len(poses_3d)):\n",
    "        poses_3d[i] = poses_3d[i] - poses_3d[i].mean(axis=0, keepdims=True)\n",
    "    \n",
    "    # 3. Convert to OpenPose format\n",
    "    print(\"Converting to OpenPose format...\")\n",
    "    control_images = pose_converter.convert_sequence(poses_3d)\n",
    "    \n",
    "    # 4. Generate video frames\n",
    "    print(\"Generating video frames...\")\n",
    "    video_frames = generate_video_frames(\n",
    "        control_images,\n",
    "        PROMPT,\n",
    "        NEGATIVE_PROMPT,\n",
    "        NUM_INFERENCE_STEPS,\n",
    "        GUIDANCE_SCALE\n",
    "    )\n",
    "    \n",
    "    # 5. Save video\n",
    "    print(f\"Saving video to: {output_path}\")\n",
    "    save_video(video_frames, output_path, fps=FPS)\n",
    "    \n",
    "    return video_frames\n",
    "\n",
    "# Example: Process a single file\n",
    "# video_frames = process_pose_file(\n",
    "#     pose_file_path=\"/content/pose_data/pose_sequence.json\",\n",
    "#     output_path=\"/content/output_video.mp4\",\n",
    "#     max_frames=16  # Limit for testing\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "process_directory"
   },
   "outputs": [],
   "source": [
    "# Process all pose files in a directory\n",
    "def process_directory(input_dir, output_dir, max_frames=None):\n",
    "    \"\"\"Process all pose files in a directory\"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all pose files\n",
    "    pose_files = []\n",
    "    for ext in ['.json', '.npy', '.npz']:\n",
    "        pose_files.extend(list(input_dir.glob(f'*{ext}')))\n",
    "    \n",
    "    pose_files.sort()\n",
    "    \n",
    "    if not pose_files:\n",
    "        print(f\"No pose files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(pose_files)} pose file(s)\")\n",
    "    \n",
    "    for pose_file in pose_files:\n",
    "        print(f\"\\nProcessing: {pose_file.name}\")\n",
    "        output_path = output_dir / f\"{pose_file.stem}_generated.mp4\"\n",
    "        \n",
    "        try:\n",
    "            process_pose_file(pose_file, output_path, max_frames=max_frames)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pose_file.name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "# Example: Process entire directory\n",
    "# process_directory(\n",
    "#     input_dir=\"/content/drive/MyDrive/dataset/violence/violence_01\",\n",
    "#     output_dir=\"/content/output_videos\",\n",
    "#     max_frames=16  # Limit for testing\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize_results"
   },
   "source": [
    "## 8. Visualize Results\n",
    "\n",
    "Display generated frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_video"
   },
   "outputs": [],
   "source": [
    "# Display generated video frames\n",
    "def display_frames(frames, num_frames=8):\n",
    "    \"\"\"Display a sample of generated frames\"\"\"\n",
    "    num_display = min(num_frames, len(frames))\n",
    "    indices = np.linspace(0, len(frames)-1, num_display, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_display, figsize=(4*num_display, 4))\n",
    "    if num_display == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        axes[i].imshow(frames[idx])\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Frame {idx+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Display frames\n",
    "# if 'video_frames' in locals():\n",
    "#     display_frames(video_frames, num_frames=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 9. Download Results\n",
    "\n",
    "Download the generated video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_video"
   },
   "outputs": [],
   "source": [
    "# Download generated video\n",
    "# from google.colab import files\n",
    "# files.download('/content/output_video.mp4')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
